{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe",
   "metadata": {},
   "source": [
    "# Word2Vec as a Model of Interactions between Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24105460-c51c-42e1-a128-f3380d969b3a",
   "metadata": {},
   "source": [
    "## Word2Vec Introduction\n",
    "\n",
    "The [word2vec](https://en.wikipedia.org/wiki/Word2vec) model was proposed to vectorize words. A word is a string. It cannot be \"computed\" on a computer. We have to encode words to vectors. One direct way of encoding is called one-hot. That is, given a vocabulary which is a list of words, the i-th word has vector $x$ with $x^i$ = 1 and all other components vanish, like $(0, \\cdots, 0, 1, 0, \\cdots, 0)$. One-hot encoding is not very efficient since its dimension equals to the vocabulary size. But the vocabulary may be quite large. This motives the idea of word2vec, that is, encoding words to dense vectors with a small dimension.\n",
    "\n",
    "The basic idea behind word2vec is modeling the probability of appearance of two given words $w_1$ and $w_2$ as a neighbour in a corpus. Given two words $w_1$ and $w_2$ with vectors $x_1$ and $x_2$ respectively, the probability of being neighbour is assumed to be\n",
    "\n",
    "$$ p_{\\text{neighbour}} (w_1, w_2) \\propto \\exp(x_1 \\cdot x_2). $$\n",
    "\n",
    "So, for word2vec model, the learning task find a vector for each word so that the $p_{\\text{neighbour}}$ fits the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd748953-3543-48d4-9e54-060f949facc9",
   "metadata": {},
   "source": [
    "## Interactions between Words\n",
    "\n",
    "If words have been one-hot encoded, then for each one-hot encoded word $w_i$, its vector is given by $x_i = W \\cdot w_i$. The matrix $W$ has dimension $(E, V)$ where $E$ represents the word-vector dimension and $V$ the vocabulary size. Then, it can be derived directly that\n",
    "\n",
    "$$ x_1 \\cdot x_2 = u^t \\cdot A \\cdot u, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ u := w_1 + w_2 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ A := W^t \\cdot W - \\textrm{diag} (W^t \\cdot W). $$\n",
    "\n",
    "The matrix $A$ has dimension $(V, V)$. It is symmetric, with vanished diagonal elements. It is recognized as a Boltzmann machine with the energy given by $E(u; A) := -(1/2) u^t \\cdot A \\cdot u$ and unit temperature. Fitting a Boltzmann machine is minimizing the loss\n",
    "\n",
    "$$ L(W) = E(w_1, w_2; W) - E(\\tilde{w}_1, \\tilde{w}_2; W), $$\n",
    "\n",
    "for any two neighboured words (one-hot encoded) $(w_1, w_2)$ and two \"fantacy\" words $(\\tilde{w}_1, \\tilde{w}_2)$. The key point is ensuring that $E(w_1, w_2; W) > E(\\tilde{w}_1, \\tilde{w}_2; W)$ is more probable than the inverse. In this way, the $W$ is adjusted so that the $(w_1, w_2)$ is going to be a local minimum of the energy.\n",
    "\n",
    "Boltzmann machine ensures this by sampling $u \\sim \\text{Bernoulli}(\\sigma(A \\cdot u))$, where $\\sigma$ is the sigmoid function. Generally, the sampled is not a two-hot vector, but multi-hot. We shall select only two word-indices from it. Notice that the greater $(A \\cdot u)_{\\alpha}$ is, the more probable we select word-index $\\alpha$. This hints for the categorical distribution\n",
    "\n",
    "$$ p_{\\alpha} = \\frac{ \\exp(\\sum_{\\beta} A_{\\alpha \\beta} u^{\\beta} / T) }{ \\sum_{\\alpha'} \\exp(\\sum_{\\beta'} A_{\\alpha' \\beta'} u^{\\beta'} / T) }, $$\n",
    "\n",
    "where $T$ is a positive number that characterizes the randomness. It is a categorical distribution with alphabet size $V$. It is indicated that we shall sample two word-indices from this distribution, as the fantacy words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632b21f-312d-45f4-b1d5-58c84143313c",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 10:24:16.277983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2 ** 12  # vocabulary size.\n",
    "BATCH_SIZE = 128  # batch size of training.\n",
    "VECTOR_DIM = 500  # dimension of word-vector\n",
    "NEIGHBOURS = 2  # window size used for generating dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1",
   "metadata": {},
   "source": [
    "The original word2vec model is trained on the \"text8\" dataset. It is preprocessed and can be found on [internet](https://mattmahoney.net/dc/text8.zip). It is a zip file. By unzipping, we get a text file named `text8`. This is the preprocessed data that we can use directly, except for excluding words with single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/text8', 'r') as f:\n",
    "    text8 = [word for word in list(f)[0].split(' ') if len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53",
   "metadata": {},
   "source": [
    "There are lots of different words in the \"text8\" text. We shall limit the vocabulary used for building our model. For this purpose, we employ the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659e8923-7f5e-409f-9799-bea21b472763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.67 s, sys: 10.8 ms, total: 1.68 s\n",
      "Wall time: 1.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counter = Counter(text8)\n",
    "vocab = {}\n",
    "for i, (word, _) in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i: w for w, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f",
   "metadata": {},
   "source": [
    "Now, we construct the collection of pairs of center word (called \"target\" in the original paper of word2vec) and its neighbour (called \"context\") in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "000eafbf-7070-4069-8a56-489981609308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 s, sys: 286 ms, total: 13.3 s\n",
      "Wall time: 13.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "targets = []\n",
    "contexts = []\n",
    "for (i, target) in enumerate(text8[NEIGHBOURS:-NEIGHBOURS]):\n",
    "    for j in range(i-NEIGHBOURS, i+NEIGHBOURS+1):\n",
    "        if j == i: continue\n",
    "        context = text8[j]\n",
    "        targets.append(target)\n",
    "        contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d3ab3-de45-4358-83bd-1bd327064634",
   "metadata": {},
   "source": [
    "While converting from word to its index in the vocabulary, we have to drop the pairs in which there is at least one word that is absent in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.5 s, sys: 222 ms, total: 13.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "target_ids = []\n",
    "context_ids = []\n",
    "for w1, w2 in zip(targets, contexts):\n",
    "    if w1 not in vocab or w2 not in vocab:\n",
    "        continue\n",
    "    target_ids.append(vocab[w1])\n",
    "    context_ids.append(vocab[w2])\n",
    "\n",
    "# List -> np.ndarray -> Dataset is much faster than List -> Dataset.\n",
    "target_ids = np.asarray(target_ids, dtype='int32')\n",
    "context_ids = np.asarray(context_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46226743,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057",
   "metadata": {},
   "source": [
    "Now, convert the processed data to TensorFlow's dataset protocol for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa50fbc-f389-4ee5-9100-d89093030da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((target_ids, context_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb",
   "metadata": {},
   "source": [
    "Let see some instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [   4  724 3255  126 3209]\n",
      "y: [288 724 892 126   1]\n"
     ]
    }
   ],
   "source": [
    "for x, y in ds.batch(5).take(1):\n",
    "    tf.print(f'x: {x}')\n",
    "    tf.print(f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10c1e3b-3080-4263-aa3d-eb7a7a80afcc",
   "metadata": {},
   "source": [
    "In addition to the \"model of interactions between words\" described previously, we have to regularize the word vectors, equivalently the matrix $W$, so that the word vectors are normalized. It helps avoid some word vector becomes too large that it dominates the interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"Word2Vec as a model of interactions between words.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Integer for the vocabulary size.\n",
    "        vector_dim: Integer for the word-vector dimension.\n",
    "        T: Positive float for the randomness in generating fantasy data.\n",
    "        test_ansatz_steps: Positve integer for a step length. If it is greater\n",
    "          than zero, then during the training, we test the ansatz that the\n",
    "          fantasy data have lower energy than the real data per `test_ansatz_steps`\n",
    "          steps.\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementation conventions:\n",
    "    # 1. The x and y employed throughout the implementation represent\n",
    "    #    word-indices. Thus they are tensors with shape [batch_size]\n",
    "    #    and dtype int32.\n",
    "    # 2. We use B for batch size, V for vocabulary size, and D for vector\n",
    "    #    dimension.\n",
    "\n",
    "    def __init__(self, vocab_size, vector_dim, T=1e-2, test_ansatz_steps=0):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.T = T\n",
    "        self.test_ansatz_steps = test_ansatz_steps\n",
    "\n",
    "        # It is convenient to use layer.Layer API to implement the W matrix.\n",
    "        # The constraint is a projection to the W after each step of gradient\n",
    "        # descent.\n",
    "        self.embed = keras.layers.Embedding(\n",
    "            vocab_size, vector_dim,\n",
    "            embeddings_constraint=keras.constraints.UnitNorm(axis=1),\n",
    "        )\n",
    "        self.embed.build([vocab_size])\n",
    "        self.W = self.embed.weights[0]  # (V, D).\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "    def energy(self, x, y):\n",
    "        return -tf.reduce_sum(self(x) * self(y), axis=1)\n",
    "\n",
    "    def sample_fantasy(self, x, y):\n",
    "        # Compute logits (the A \\cdot u, where u := x + y)\n",
    "        z = self(x) + self(y)  # (B, D)\n",
    "        raw_logits = tf.matmul(z, tf.transpose(self.W))  # (B, V)\n",
    "        indices = tf.stack([tf.range(tf.shape(x)[0]), x], axis=1)\n",
    "        update = tf.zeros(tf.shape(x))\n",
    "        logits = tf.tensor_scatter_nd_update(raw_logits, indices, update)\n",
    "\n",
    "        # Sample two samples by probability proportional to `exp(logits / self.T)`.\n",
    "        samples = tf.random.categorical((1/self.T) * logits, 2, dtype=tf.int32)\n",
    "        return tf.unstack(samples, axis=1)\n",
    "\n",
    "    def loss(self, real, fantacy):\n",
    "        return tf.reduce_mean(\n",
    "            self.energy(real[0], real[1]) -\n",
    "            self.energy(fantacy[0], fantacy[1])\n",
    "        )\n",
    "\n",
    "    def test_ansatz(self, real, fantacy):\n",
    "        \"\"\"The result ratio shall be no less than a half. As the training\n",
    "        continues, this ratio will gradually decreases and close to 0.5.\n",
    "        \"\"\"\n",
    "        ansatz = (\n",
    "            self.energy(real[0], real[1]) >\n",
    "            self.energy(fantacy[0], fantacy[1])\n",
    "        )\n",
    "        ansatz_correct_ratio = tf.reduce_mean(tf.cast(ansatz, tf.float32))\n",
    "        return ansatz_correct_ratio\n",
    "\n",
    "    def get_train_step(self, optimizer):\n",
    "        step = tf.Variable(0, dytpe=tf.int32)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x, y):\n",
    "            \"\"\"The x and y are word-indices, tensors with shape [batch_size]\n",
    "            and dtype int32.\n",
    "            \"\"\"\n",
    "            real = (x, y)\n",
    "            fantasy = self.sample_fantasy(x, y)\n",
    "\n",
    "            # Compute loss and its gradient, and optimize.\n",
    "            # The gradient to the weights in embedding layer is treated as sparse,\n",
    "            # Convert sparse to dense for optimizer.\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss_value = self.loss(real, fantasy)\n",
    "            grads = tf.convert_to_tensor(tape.gradient(loss_value, self.W))\n",
    "            optimizer.apply_gradients([(grads, self.W)])\n",
    "\n",
    "            # Test ansatz if needed\n",
    "            if self.test_ansatz_steps and step % self.test_ansatz_steps == 0:\n",
    "                ansatz_correct_ratio = self.test_ansatz(real, fantasy)\n",
    "                tf.print(f'\\nansatz correct sample ratio: {ansatz_correct_ratio}\\n')\n",
    "\n",
    "            step.assign_add(1)\n",
    "            return loss_value\n",
    "\n",
    "        return train_step, step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
    "# optimizer = keras.optimizers.gradient_descent_v2.SGD()\n",
    "optimizer = keras.optimizers.adam_v2.Adam()\n",
    "train_step, step = model.get_train_step(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d2375681-b393-4c67-8a20-730910a6a428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37587/361147 [==>...........................] - ETA: 54:00 - loss: 0.0431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pbar \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mgeneric_utils\u001b[38;5;241m.\u001b[39mProgbar(\u001b[38;5;28mlen\u001b[39m(ds\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m10000\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE):\n\u001b[0;32m----> 3\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(tf\u001b[38;5;241m.\u001b[39mcast(step, tf\u001b[38;5;241m.\u001b[39mfloat32), values\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, loss_value)])\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
      "File \u001b[0;32m/nix/store/xpfh5nb3dhspkm67a5b8ag17ylrw1fps-python3.11-tensorflow-2.13.0/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = keras.utils.generic_utils.Progbar(len(ds.batch(BATCH_SIZE)))\n",
    "for x, y in ds.shuffle(10000).batch(BATCH_SIZE):\n",
    "    loss_value = train_step(x, y)\n",
    "    pbar.update(tf.cast(step, tf.float32), values=[('loss', loss_value)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe99625-f249-4653-9650-ed946d7af46a",
   "metadata": {},
   "source": [
    "Since the word-vectors are all normalized, it is natural to consider angular distance as a measurement of the relation between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_k(model, vector, k):\n",
    "    z = tf.convert_to_tensor([vector])  # (1, D)\n",
    "    distances = tf.math.acos(tf.matmul(z, tf.transpose(model.W)))  # (1, V)\n",
    "    _, top_ids = tf.math.top_k(-distances, k=k)\n",
    "    return top_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world: world, war, century, prime, states\n",
      "\n",
      "boy: boy, young, poor, former, regard\n",
      "\n",
      "happy: happy, month, jackson, legislation, canon\n",
      "\n",
      "zero: zero, two, three, nine, seven\n",
      "\n",
      "sun: sun, evil, begins, marine, read\n",
      "\n",
      "football: football, baseball, world, absolute, links\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ('world', 'boy', 'happy', 'zero', 'sun', 'football'):\n",
    "    closest_indices = get_closest_k(model, model(vocab[word]), 5)\n",
    "    print(f'{word}: {\", \".join([id_to_word[idx] for idx in closest_indices[0,:]])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f",
   "metadata": {},
   "source": [
    "From this simple evaluation, it has been found that the word2vec re-implemented from the aspect of interaction reveals some deeper relations of words.\n",
    "\n",
    "In addition, we drop the contribution in the loss from the fantasy data, the training fails in such a way that only the most frequent words (like \"the\", \"of\", and \"in\") appear as the closest for any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e3e11-9f51-4ec7-a8b0-f9eca47f1597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
