{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe",
      "metadata": {
        "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe"
      },
      "source": [
        "# Word2Vec as a Model of Interactions between Words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24105460-c51c-42e1-a128-f3380d969b3a",
      "metadata": {
        "id": "24105460-c51c-42e1-a128-f3380d969b3a"
      },
      "source": [
        "## Word2Vec Introduction\n",
        "\n",
        "The [word2vec](https://en.wikipedia.org/wiki/Word2vec) model was proposed to vectorize words. A word is a string. It cannot be \"computed\" on a computer. We have to encode words to vectors. One direct way of encoding is called one-hot. That is, given a vocabulary which is a list of words, the i-th word has vector $x$ with $x^i$ = 1 and all other components vanish, like $(0, \\cdots, 0, 1, 0, \\cdots, 0)$. One-hot encoding is not very efficient since its dimension equals to the vocabulary size. But the vocabulary may be quite large. This motives the idea of word2vec, that is, encoding words to dense vectors with a small dimension.\n",
        "\n",
        "The basic idea behind word2vec is modeling the probability of appearance of two given words $w_1$ and $w_2$ as a neighbour in a corpus. Given two words $w_1$ and $w_2$ with vectors $x_1$ and $x_2$ respectively, the probability of being neighbour is assumed to be\n",
        "\n",
        "$$ p_{\\text{neighbour}} (w_1, w_2) \\propto \\exp(x_1 \\cdot x_2). $$\n",
        "\n",
        "So, for word2vec model, the learning task find a vector for each word so that the $p_{\\text{neighbour}}$ fits the real data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd748953-3543-48d4-9e54-060f949facc9",
      "metadata": {
        "id": "bd748953-3543-48d4-9e54-060f949facc9"
      },
      "source": [
        "## Word2Vec as Interactions between Words\n",
        "\n",
        "If words have been one-hot encoded, then for each one-hot encoded word $w_i$, its vector is given by $x_i = W \\cdot w_i$. The matrix $W$ has dimension $(E, V)$ where $E$ represents the word-vector dimension and $V$ the vocabulary size. Then, it can be derived directly that\n",
        "\n",
        "$$ x_1 \\cdot x_2 = \\frac{1}{2} u^t \\cdot A \\cdot u, $$\n",
        "\n",
        "where\n",
        "\n",
        "$$ u := w_1 + w_2 $$\n",
        "\n",
        "and\n",
        "\n",
        "$$ A := W^t \\cdot W - \\textrm{diag} (W^t \\cdot W). $$\n",
        "\n",
        "The matrix $A$ has dimension $(V, V)$. It is symmetric, with vanished diagonal elements. It is recognized as a Boltzmann machine with the energy given by $E(u; A) := -(1/2) u^t \\cdot A \\cdot u$ and unit temperature. Fitting a Boltzmann machine is minimizing the loss\n",
        "\n",
        "$$ L(W) = E(w_1, w_2; W) - E(\\tilde{w}_1, \\tilde{w}_2; W), $$\n",
        "\n",
        "for any two neighboured words (one-hot encoded) $(w_1, w_2)$ and two \"fantasy\" words $(\\tilde{w}_1, \\tilde{w}_2)$. In this expression, we have re-write $E$ in $w$s and $W$. The key point is ensuring that $E(w_1, w_2; W) > E(\\tilde{w}_1, \\tilde{w}_2; W)$ is more probable than the inverse. In this way, the $W$ is adjusted so that the $(w_1, w_2)$ is going to be a local minimum of the energy.\n",
        "\n",
        "Boltzmann machine ensures this by sampling $u' \\sim \\text{Bernoulli}(\\sigma(A \\cdot u))$, where $\\sigma$ is the sigmoid function. Generally, the sampled is a multi-hot vector, representing a collection of different word-indices, $(w_{i_1}, \\ldots, w_{i_K})$. The energy of $u'$ then comes to be\n",
        "\n",
        "$$ E(u'; A) = -\\frac{1}{2} u^t \\cdot A \\cdot u = \\sum_{i_m} \\sum_{i_n \\neq i_m}  E(\\tilde{w}_{i_m}, \\tilde{w}_{i_n}; W). $$\n",
        "\n",
        "The summation runs over all the pairs of $(\\tilde{w}_{i_m}, \\tilde{w}_{i_n})$ where $i_m$ and $i_n$ are distinct. The pairs may become too many to compute. In addition, the $K$ varies for each datum. So, it is hard to compute in TensorFlow (we will employ) wherein tensor shapes are static. We shall select a fixed number of pairs from them, while ensuring that the fantasy energy is no less than the real energy. The greater the $(A \\cdot u)_k$ is for some word $w_k$, the more probable it will be sampled. So, we are to select a pair in which words are more likely being sampled. To do this, we consider the categorical distribution\n",
        "\n",
        "$$ p_{\\alpha} = \\frac{ \\exp(\\sum_{\\beta} A_{\\alpha \\beta} u^{\\beta} / T) }{ \\sum_{\\alpha'} \\exp(\\sum_{\\beta'} A_{\\alpha' \\beta'} u^{\\beta'} / T) }, $$\n",
        "\n",
        "where $T$ is a positive number that characterizes the randomness. It is a categorical distribution with alphabet size $V$. It is indicated that we shall sample two word-indices based on this distribution as the selected pair. If the fantasy energy is no less than the real energy, then the loss will be non-negative.\n",
        "\n",
        "In addition, the component $(A \\cdot u)_k$ can be reduced to\n",
        "\n",
        "$$ ( z_{k m} - \\delta_{k m} z_{k m} ) + ( z_{k n} - \\delta_{k n} z_{k n} ), $$\n",
        "\n",
        "where $z_{k m}$ represents the inner product between word-vectors of $x_k$ and $x_m$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4632b21f-312d-45f4-b1d5-58c84143313c",
      "metadata": {
        "id": "4632b21f-312d-45f4-b1d5-58c84143313c"
      },
      "source": [
        "## Global Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123",
      "metadata": {
        "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import keras\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5bc68c-588c-42d2-acd3-dd4c7b3bfa58",
      "metadata": {
        "id": "9d5bc68c-588c-42d2-acd3-dd4c7b3bfa58"
      },
      "source": [
        "Configurations are given as follow. The vector dimension employed by the original word2vec paper is `300`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481",
      "metadata": {
        "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481"
      },
      "outputs": [],
      "source": [
        "NEIGHBOURS = 2  # window size used for generating dataset.\n",
        "VOCAB_SIZE = 2 ** 14  # vocabulary size.\n",
        "VECTOR_DIM = 300  # dimension of word-vector.\n",
        "BATCH_SIZE = 128  # batch size of training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b622b27f-50bd-418b-9cc7-768b1326bf0b",
      "metadata": {
        "id": "b622b27f-50bd-418b-9cc7-768b1326bf0b"
      },
      "source": [
        "In addition, we add other global parameters as switchers for several experiments. We will test the case where $L(W) = E(w_1, w_2; W)$, that is, the fantasy energy is not involved. Also the case where fantasy data is sampled randomly and uniformly from the vocabulary. The results are summarized in the conclusion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "08cec2bd-16ad-40a1-9ab6-7c7b26f68684",
      "metadata": {
        "id": "08cec2bd-16ad-40a1-9ab6-7c7b26f68684"
      },
      "outputs": [],
      "source": [
        "TEST_WITHOUT_FANTASY_ENERGY = False\n",
        "TEST_UNIFORM_SAMPLE_FANTASY = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061",
      "metadata": {
        "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061"
      },
      "source": [
        "## Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1",
      "metadata": {
        "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1"
      },
      "source": [
        "The original word2vec model is trained on the \"text8\" dataset. It is preprocessed and can be found on [internet](https://mattmahoney.net/dc/text8.zip). It is a zip file. By unzipping, we get a text file named `text8`. This is the preprocessed data that we can use directly, except for excluding words with single character."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -f text8 ]; then\n",
        "    wget  https://mattmahoney.net/dc/text8.zip\n",
        "    unzip text8.zip\n",
        "    rm text8.zip\n",
        "fi"
      ],
      "metadata": {
        "id": "xih9IHYPlSuh"
      },
      "id": "xih9IHYPlSuh",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3",
      "metadata": {
        "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3"
      },
      "outputs": [],
      "source": [
        "with open('text8', 'r') as f:\n",
        "    text8 = [word for word in list(f)[0].split(' ') if len(word) > 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53",
      "metadata": {
        "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53"
      },
      "source": [
        "There are lots of different words in the \"text8\" text. We shall limit the vocabulary used for building our model. For this purpose, we employ the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "659e8923-7f5e-409f-9799-bea21b472763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "659e8923-7f5e-409f-9799-bea21b472763",
        "outputId": "a01a3caa-8499-45a9-8b00-d8732c485120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.88 s, sys: 47.8 ms, total: 3.93 s\n",
            "Wall time: 5.47 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "counter = Counter(text8)\n",
        "vocab = {}\n",
        "for i, (word, _) in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "    vocab[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed",
      "metadata": {
        "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed"
      },
      "outputs": [],
      "source": [
        "id_to_word = {i: w for w, i in vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f",
      "metadata": {
        "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f"
      },
      "source": [
        "Now, we construct the collection of pairs of center word (called \"target\" in the original paper of word2vec) and its neighbour (called \"context\") in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "000eafbf-7070-4069-8a56-489981609308",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000eafbf-7070-4069-8a56-489981609308",
        "outputId": "1e1e0244-9ea3-488f-ddee-dfc604479649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27.9 s, sys: 671 ms, total: 28.6 s\n",
            "Wall time: 30.4 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "targets = []\n",
        "contexts = []\n",
        "for (i, target) in enumerate(text8[NEIGHBOURS:-NEIGHBOURS]):\n",
        "    for j in range(i-NEIGHBOURS, i+NEIGHBOURS+1):\n",
        "        if j == i: continue\n",
        "        context = text8[j]\n",
        "        targets.append(target)\n",
        "        contexts.append(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667d3ab3-de45-4358-83bd-1bd327064634",
      "metadata": {
        "id": "667d3ab3-de45-4358-83bd-1bd327064634"
      },
      "source": [
        "While converting from word to its index in the vocabulary, we have to drop the pairs in which there is at least one word that is absent in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
        "outputId": "ab48d774-09e5-499a-ac47-00781b8d7bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 29.8 s, sys: 674 ms, total: 30.4 s\n",
            "Wall time: 30.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "target_ids = []\n",
        "context_ids = []\n",
        "for w1, w2 in zip(targets, contexts):\n",
        "    if w1 not in vocab or w2 not in vocab:\n",
        "        continue\n",
        "    target_ids.append(vocab[w1])\n",
        "    context_ids.append(vocab[w2])\n",
        "\n",
        "# List -> np.ndarray -> Dataset is much faster than List -> Dataset.\n",
        "target_ids = np.asarray(target_ids, dtype='int32')\n",
        "context_ids = np.asarray(context_ids, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
        "outputId": "8e6fdc51-fdf1-41f1-8b6a-037731b703ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((57706802,), (57706802,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "target_ids.shape, context_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057",
      "metadata": {
        "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057"
      },
      "source": [
        "Now, convert the processed data to TensorFlow's dataset protocol for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3aa50fbc-f389-4ee5-9100-d89093030da0",
      "metadata": {
        "id": "3aa50fbc-f389-4ee5-9100-d89093030da0"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((target_ids, context_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb",
      "metadata": {
        "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb"
      },
      "source": [
        "Let see some instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
        "outputId": "3c977c60-ff47-4008-98c8-2e9dac7d9016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  [10 10 10 10 180]\n",
            "y:  [19 13 3054 10 13]\n"
          ]
        }
      ],
      "source": [
        "for x, y in ds.batch(5).take(1):\n",
        "    tf.print('x: ', x)\n",
        "    tf.print('y: ', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9",
      "metadata": {
        "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc",
      "metadata": {
        "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    \"\"\"Word2Vec as a model of interactions between words.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Integer for the vocabulary size.\n",
        "        vector_dim: Integer for the word-vector dimension.\n",
        "        T: Positive float for the randomness in generating fantasy data.\n",
        "    \"\"\"\n",
        "\n",
        "    # We use B for batch size, V for vocabulary size, and D for vector dimension.\n",
        "\n",
        "    def __init__(self, vocab_size, vector_dim, T=1e-3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vector_dim = vector_dim\n",
        "        self.T = T\n",
        "\n",
        "        W_init = tf.random.uniform([vocab_size, vector_dim], dtype=tf.float32,\n",
        "                                   minval=-0.05, maxval=0.05)\n",
        "        self.W = tf.Variable(W_init)  # (V, D)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return tf.nn.embedding_lookup(self.W, x)\n",
        "\n",
        "    def energy(self, pair):\n",
        "        x, y = pair\n",
        "        return -tf.reduce_sum(self(x) * self(y), axis=1)\n",
        "\n",
        "    def sample_fantasy(self, pair):\n",
        "        if TEST_UNIFORM_SAMPLE_FANTASY:\n",
        "            batch_size = tf.shape(pair[0])[0]\n",
        "            samples = tf.random.uniform(\n",
        "                shape=[batch_size, 2], maxval=self.vocab_size, dtype=tf.int32)\n",
        "            return tf.unstack(samples, axis=1)\n",
        "\n",
        "        def get_logits(x):\n",
        "            raw_logits = tf.matmul(self(x), tf.transpose(self.W))  # (B, V)\n",
        "            indices = tf.stack([tf.range(tf.shape(x)[0]), x], axis=1)\n",
        "            update = tf.zeros(tf.shape(x))\n",
        "            return tf.tensor_scatter_nd_update(raw_logits, indices, update)\n",
        "        x, y = pair\n",
        "        logits = get_logits(x) + get_logits(y)  # (B, V)\n",
        "        # Sample two samples by probability proportional to `exp(logits / self.T)`.\n",
        "        samples = tf.random.categorical((logits / self.T), 2, dtype=tf.int32)\n",
        "        return tf.unstack(samples, axis=1)\n",
        "\n",
        "    def loss(self, real_pair, fantasy_pair):\n",
        "        if TEST_WITHOUT_FANTASY_ENERGY:\n",
        "            return tf.reduce_mean(self.energy(real_pair))\n",
        "\n",
        "        return tf.reduce_mean(self.energy(real_pair) - self.energy(fantasy_pair))\n",
        "\n",
        "    def get_train_step(self, optimizer):\n",
        "        step = tf.Variable(0, dytpe=tf.int32)\n",
        "\n",
        "        @tf.function\n",
        "        def train_step(real_pair):\n",
        "            fantasy_pair = self.sample_fantasy(real_pair)\n",
        "\n",
        "            # Compute loss and its gradient, and optimize.\n",
        "            with tf.GradientTape() as gt:\n",
        "                loss_value = self.loss(real_pair, fantasy_pair)\n",
        "            grads = gt.gradient(loss_value, self.W)\n",
        "            # The gradient to the weights in embedding layer is treated as sparse,\n",
        "            # Convert sparse to dense for optimizer.\n",
        "            grads = tf.convert_to_tensor(grads)\n",
        "            optimizer.apply_gradients([(grads, self.W)])\n",
        "\n",
        "            step.assign_add(1)\n",
        "            return loss_value\n",
        "\n",
        "        return train_step, step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655",
      "metadata": {
        "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf",
      "metadata": {
        "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
        "optimizer = keras.optimizers.gradient_descent_v2.SGD()\n",
        "train_step, step = model.get_train_step(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a7441b1c-eef8-484f-8533-f92b4bca2917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7441b1c-eef8-484f-8533-f92b4bca2917",
        "outputId": "0187a152-fccd-49b6-b710-6132413c80c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "450835/450835 [==============================] - 2239s 5ms/step - loss: 0.0586\n"
          ]
        }
      ],
      "source": [
        "process_bar = keras.utils.generic_utils.Progbar(len(ds.batch(BATCH_SIZE)))\n",
        "for real_pair in ds.shuffle(10000).batch(BATCH_SIZE):\n",
        "    loss_value = train_step(real_pair)\n",
        "    process_bar.update(current=tf.cast(step, tf.float32),\n",
        "                       values=[('loss', loss_value)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c66368ad-3d34-4f61-b4fb-8f30b764bd0b",
      "metadata": {
        "id": "c66368ad-3d34-4f61-b4fb-8f30b764bd0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bef881-9cbe-41b8-a042-fcb8e61123e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(16384, 300) dtype=float32, numpy=\n",
              "array([[-1.52643174e-02,  1.53087750e-02,  5.68150263e-03, ...,\n",
              "        -1.95565750e-03,  8.43332428e-03,  3.50515801e-03],\n",
              "       [-2.05261614e-02,  7.04018353e-03,  2.99878791e-03, ...,\n",
              "        -2.86304182e-03, -5.76683437e-04,  6.45227777e-03],\n",
              "       [-1.05065675e-02,  7.78524857e-03, -3.08963936e-05, ...,\n",
              "        -6.58661174e-03,  2.18126252e-02,  8.18720181e-03],\n",
              "       ...,\n",
              "       [ 1.33575173e-02, -1.60282943e-02,  1.47384610e-02, ...,\n",
              "        -1.80412363e-02, -1.86773334e-02,  1.88402813e-02],\n",
              "       [-3.38749439e-02, -1.52752735e-02, -1.36089744e-02, ...,\n",
              "         2.59806234e-02,  4.37434763e-02,  4.09261100e-02],\n",
              "       [-3.05379201e-02, -2.03030510e-03,  6.38827449e-03, ...,\n",
              "         6.64780149e-04, -4.22020033e-02, -1.93954725e-02]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc",
      "metadata": {
        "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe99625-f249-4653-9650-ed946d7af46a",
      "metadata": {
        "id": "5fe99625-f249-4653-9650-ed946d7af46a"
      },
      "source": [
        "To evalute the model, we consider the $k$ words that are closest to a given word. The relation between these words shall be meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8",
      "metadata": {
        "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8"
      },
      "outputs": [],
      "source": [
        "def get_closest_k(model, word, k):\n",
        "    vector = model([vocab[word]])  # (1, D)\n",
        "    distances = tf.matmul(vector, tf.transpose(model.W))  # (1, V)\n",
        "    _, top_ids = tf.math.top_k(distances, k=k)\n",
        "    return top_ids.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
      "metadata": {
        "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48fdbb5f-e0c6-4bd1-9d0e-ab19995e7143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world: world, war, international, between, us\n",
            "\n",
            "boy: boy, filed, lighthouse, amplifier, kilogram\n",
            "\n",
            "zero: zero, two, five, one, isbn\n",
            "\n",
            "sun: sun, seemingly, janet, unpleasant, attention\n",
            "\n",
            "music: music, classical, between, desired, garden\n",
            "\n",
            "the: the, of, and, that, in\n",
            "\n",
            "to: to, of, be, he, according\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for word in ('world', 'boy', 'zero', 'sun', 'music', 'the', 'to'):\n",
        "    closest_indices = get_closest_k(model, word, 5)\n",
        "    print(f'{word}: {\", \".join([id_to_word[idx] for idx in closest_indices[0,:]])}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e82c7ea-a7a8-4e32-b281-1a28b5764271",
      "metadata": {
        "id": "9e82c7ea-a7a8-4e32-b281-1a28b5764271"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f",
      "metadata": {
        "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f"
      },
      "source": [
        "From this simple evaluation, it has been found that the word2vec re-implemented from the aspect of interaction reveals some deeper relations of words.\n",
        "\n",
        "If we drop the contribution in the loss from the fantasy data, the training fails in such a way that only the most frequent words (like \"the\", \"of\", and \"in\") appear as the closest for any word.\n",
        "\n",
        "If we sample fantasy data by uniform sampler, the training also fails in the previous way."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}