{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe",
      "metadata": {
        "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe"
      },
      "source": [
        "# Word2Vec as a Model of Interactions between Words"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24105460-c51c-42e1-a128-f3380d969b3a",
      "metadata": {
        "id": "24105460-c51c-42e1-a128-f3380d969b3a"
      },
      "source": [
        "## Word2Vec Introduction\n",
        "\n",
        "The [word2vec](https://en.wikipedia.org/wiki/Word2vec) model was proposed to vectorize words. A word is a string. It cannot be \"computed\" on a computer. We have to encode words to vectors. One direct way of encoding is called one-hot. That is, given a vocabulary which is a list of words, the i-th word has vector $x$ with $x^i$ = 1 and all other components vanish, like $(0, \\cdots, 0, 1, 0, \\cdots, 0)$. One-hot encoding is not very efficient since its dimension equals to the vocabulary size. But the vocabulary may be quite large. This motives the idea of word2vec, that is, encoding words to dense vectors with a small dimension.\n",
        "\n",
        "The basic idea behind word2vec is modeling the probability of appearance of two given words $w_1$ and $w_2$ as a neighbour in a corpus. Given two words $w_1$ and $w_2$ with vectors $x_1$ and $x_2$ respectively, the probability of being neighbour is assumed to be\n",
        "\n",
        "$$ p_{\\text{neighbour}} (w_1, w_2) \\propto \\exp(x_1 \\cdot x_2). $$\n",
        "\n",
        "So, for word2vec model, the learning task find a vector for each word so that the $p_{\\text{neighbour}}$ fits the real data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd748953-3543-48d4-9e54-060f949facc9",
      "metadata": {
        "id": "bd748953-3543-48d4-9e54-060f949facc9"
      },
      "source": [
        "## Word2Vec as Interactions between Words\n",
        "\n",
        "If words have been one-hot encoded, then for each one-hot encoded word $w_i$, its vector is given by $x_i = W \\cdot w_i$. The matrix $W$ has dimension $(E, V)$ where $E$ represents the word-vector dimension and $V$ the vocabulary size. Then, it can be derived directly that\n",
        "\n",
        "$$ x_1 \\cdot x_2 = \\frac{1}{2} u^t \\cdot A \\cdot u, $$\n",
        "\n",
        "where\n",
        "\n",
        "$$ u := w_1 + w_2 $$\n",
        "\n",
        "and\n",
        "\n",
        "$$ A := W^t \\cdot W - \\textrm{diag} (W^t \\cdot W). $$\n",
        "\n",
        "The matrix $A$ has dimension $(V, V)$. It is symmetric, with vanished diagonal elements. It is recognized as a Boltzmann machine with the energy given by $E(u; A) := -(1/2) u^t \\cdot A \\cdot u$ and unit temperature. Fitting a Boltzmann machine is minimizing the loss\n",
        "\n",
        "$$ L(W) = E(w_1, w_2; W) - E(\\tilde{w}_1, \\tilde{w}_2; W), $$\n",
        "\n",
        "for any two neighboured words (one-hot encoded) $(w_1, w_2)$ and two \"fantasy\" words $(\\tilde{w}_1, \\tilde{w}_2)$. The key point is ensuring that $E(w_1, w_2; W) > E(\\tilde{w}_1, \\tilde{w}_2; W)$ is more probable than the inverse. In this way, the $W$ is adjusted so that the $(w_1, w_2)$ is going to be a local minimum of the energy.\n",
        "\n",
        "Boltzmann machine ensures this by sampling $u' \\sim \\text{Bernoulli}(\\sigma(A \\cdot u))$, where $\\sigma$ is the sigmoid function. Generally, the sampled is a multi-hot vector, representing a collection of different word-indices, $(w_{i_1}, \\ldots, w_{i_K})$. The energy of $u'$ then comes to be\n",
        "\n",
        "$$ E(u'; A) = -\\frac{1}{2} u^t \\cdot A \\cdot u = \\sum_{i_m} \\sum_{i_n \\neq i_m}  E(\\tilde{w}_{i_m}, \\tilde{w}_{i_n}; W). $$\n",
        "\n",
        "The summation runs over all the pairs of $(\\tilde{w}_{i_m}, \\tilde{w}_{i_n})$ where $i_m$ and $i_n$ are distinct. The pairs may become too many to compute. In addition, the $K$ varies for each datum. So, it is hard to compute in TensorFlow (we will employ) wherein tensor shapes are static. We shall select a fixed number of pairs from them, while ensuring that the fantasy energy is no less than the real energy. The greater the $(A \\cdot u)_k$ is for some word $w_k$, the more probable it will be sampled. So, we are to select a pair in which words are more likely being sampled. To do this, we consider the categorical distribution\n",
        "\n",
        "$$ p_{\\alpha} = \\frac{ \\exp(\\sum_{\\beta} A_{\\alpha \\beta} u^{\\beta} / T) }{ \\sum_{\\alpha'} \\exp(\\sum_{\\beta'} A_{\\alpha' \\beta'} u^{\\beta'} / T) }, $$\n",
        "\n",
        "where $T$ is a positive number that characterizes the randomness. It is a categorical distribution with alphabet size $V$. It is indicated that we shall sample two word-indices based on this distribution as the selected pair. If the fantasy energy does be no less than the real energy, the loss will be non-negative.\n",
        "\n",
        "In addition, the component $(A \\cdot u)_k$ can be reduced to\n",
        "\n",
        "$$ ( z_{k m} - \\delta_{k m} z_{k m} ) + ( z_{k n} - \\delta_{k n} z_{k n} ), $$\n",
        "\n",
        "where $z_{k m}$ represents the inner product between word-vectors of $x_k$ and $x_m$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4632b21f-312d-45f4-b1d5-58c84143313c",
      "metadata": {
        "id": "4632b21f-312d-45f4-b1d5-58c84143313c"
      },
      "source": [
        "## Global Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123",
      "metadata": {
        "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.python import keras\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d5bc68c-588c-42d2-acd3-dd4c7b3bfa58",
      "metadata": {
        "id": "9d5bc68c-588c-42d2-acd3-dd4c7b3bfa58"
      },
      "source": [
        "Configurations are given as follow. The vocabulary size greatly affects the training speed, thus shall be tuned smaller if training becomes slow. The vector dimension employed by the original word2vec paper is `300`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481",
      "metadata": {
        "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481"
      },
      "outputs": [],
      "source": [
        "NEIGHBOURS = 2  # window size used for generating dataset.\n",
        "VOCAB_SIZE = 2 ** 14  # vocabulary size.\n",
        "VECTOR_DIM = 300  # dimension of word-vector.\n",
        "BATCH_SIZE = 128  # batch size of training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b622b27f-50bd-418b-9cc7-768b1326bf0b",
      "metadata": {
        "id": "b622b27f-50bd-418b-9cc7-768b1326bf0b"
      },
      "source": [
        "In addition, we add other global parameters as switchers for several experiments. We will test the case where $L(W) = E(w_1, w_2; W)$, that is, the fantasy energy is not involved. Also the case where fantasy data is sampled randomly and uniformly from the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "08cec2bd-16ad-40a1-9ab6-7c7b26f68684",
      "metadata": {
        "id": "08cec2bd-16ad-40a1-9ab6-7c7b26f68684"
      },
      "outputs": [],
      "source": [
        "TEST_WITHOUT_FANTASY_ENERGY = False\n",
        "TEST_UNIFORM_SAMPLE_FANTASY = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061",
      "metadata": {
        "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061"
      },
      "source": [
        "## Text Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1",
      "metadata": {
        "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1"
      },
      "source": [
        "The original word2vec model is trained on the \"text8\" dataset. It is preprocessed and can be found on [internet](https://mattmahoney.net/dc/text8.zip). It is a zip file. By unzipping, we get a text file named `text8`. This is the preprocessed data that we can use directly, except for excluding words with single character."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "if [ ! -f text8 ]; then\n",
        "    wget  https://mattmahoney.net/dc/text8.zip\n",
        "    unzip text8.zip\n",
        "    rm text8.zip\n",
        "fi"
      ],
      "metadata": {
        "id": "xih9IHYPlSuh"
      },
      "id": "xih9IHYPlSuh",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3",
      "metadata": {
        "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3"
      },
      "outputs": [],
      "source": [
        "with open('text8', 'r') as f:\n",
        "    text8 = [word for word in list(f)[0].split(' ') if len(word) > 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53",
      "metadata": {
        "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53"
      },
      "source": [
        "There are lots of different words in the \"text8\" text. We shall limit the vocabulary used for building our model. For this purpose, we employ the most frequent words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "659e8923-7f5e-409f-9799-bea21b472763",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "659e8923-7f5e-409f-9799-bea21b472763",
        "outputId": "bdfd546a-d380-45c7-df5f-f9dc0f936c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.79 s, sys: 46.3 ms, total: 3.84 s\n",
            "Wall time: 4.42 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "counter = Counter(text8)\n",
        "vocab = {}\n",
        "for i, (word, _) in enumerate(counter.most_common(VOCAB_SIZE)):\n",
        "    vocab[word] = i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed",
      "metadata": {
        "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed"
      },
      "outputs": [],
      "source": [
        "id_to_word = {i: w for w, i in vocab.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f",
      "metadata": {
        "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f"
      },
      "source": [
        "Now, we construct the collection of pairs of center word (called \"target\" in the original paper of word2vec) and its neighbour (called \"context\") in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "000eafbf-7070-4069-8a56-489981609308",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "000eafbf-7070-4069-8a56-489981609308",
        "outputId": "7296c270-8f7d-4c3b-b767-db9034c84a47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 30.2 s, sys: 737 ms, total: 31 s\n",
            "Wall time: 34.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "targets = []\n",
        "contexts = []\n",
        "for (i, target) in enumerate(text8[NEIGHBOURS:-NEIGHBOURS]):\n",
        "    for j in range(i-NEIGHBOURS, i+NEIGHBOURS+1):\n",
        "        if j == i: continue\n",
        "        context = text8[j]\n",
        "        targets.append(target)\n",
        "        contexts.append(context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "667d3ab3-de45-4358-83bd-1bd327064634",
      "metadata": {
        "id": "667d3ab3-de45-4358-83bd-1bd327064634"
      },
      "source": [
        "While converting from word to its index in the vocabulary, we have to drop the pairs in which there is at least one word that is absent in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
        "outputId": "3a69a10b-4a69-4468-da48-816c911c1583"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 34.7 s, sys: 805 ms, total: 35.5 s\n",
            "Wall time: 40 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "target_ids = []\n",
        "context_ids = []\n",
        "for w1, w2 in zip(targets, contexts):\n",
        "    if w1 not in vocab or w2 not in vocab:\n",
        "        continue\n",
        "    target_ids.append(vocab[w1])\n",
        "    context_ids.append(vocab[w2])\n",
        "\n",
        "# List -> np.ndarray -> Dataset is much faster than List -> Dataset.\n",
        "target_ids = np.asarray(target_ids, dtype='int32')\n",
        "context_ids = np.asarray(context_ids, dtype='int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
        "outputId": "1ec74be9-2266-4848-8170-e2f0c7b3bffd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((57706802,), (57706802,))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "target_ids.shape, context_ids.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057",
      "metadata": {
        "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057"
      },
      "source": [
        "Now, convert the processed data to TensorFlow's dataset protocol for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3aa50fbc-f389-4ee5-9100-d89093030da0",
      "metadata": {
        "id": "3aa50fbc-f389-4ee5-9100-d89093030da0"
      },
      "outputs": [],
      "source": [
        "ds = tf.data.Dataset.from_tensor_slices((target_ids, context_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb",
      "metadata": {
        "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb"
      },
      "source": [
        "Let see some instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
        "outputId": "f5820ffa-b1c3-4755-d887-6d730eed4725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x:  [10 10 10 10 180]\n",
            "y:  [19 13 3054 10 13]\n"
          ]
        }
      ],
      "source": [
        "for x, y in ds.batch(5).take(1):\n",
        "    tf.print('x: ', x)\n",
        "    tf.print('y: ', y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9",
      "metadata": {
        "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9"
      },
      "source": [
        "## Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc",
      "metadata": {
        "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    \"\"\"Word2Vec as a model of interactions between words.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: Integer for the vocabulary size.\n",
        "        vector_dim: Integer for the word-vector dimension.\n",
        "        T: Positive float for the randomness in generating fantasy data.\n",
        "    \"\"\"\n",
        "\n",
        "    # We use B for batch size, V for vocabulary size, and D for vector dimension.\n",
        "\n",
        "    def __init__(self, vocab_size, vector_dim, T=1e-3):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vector_dim = vector_dim\n",
        "        self.T = T\n",
        "\n",
        "        W_init = tf.random.uniform([vocab_size, vector_dim], dtype=tf.float32,\n",
        "                                   minval=-0.05, maxval=0.05)\n",
        "        self.W = tf.Variable(W_init)  # (V, D)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return tf.nn.embedding_lookup(self.W, x)\n",
        "\n",
        "    def energy(self, pair):\n",
        "        x, y = pair\n",
        "        return -tf.reduce_sum(self(x) * self(y), axis=1)\n",
        "\n",
        "    def sample_fantasy(self, pair):\n",
        "        if TEST_UNIFORM_SAMPLE_FANTASY:\n",
        "            batch_size = tf.shape(pair[0])[0]\n",
        "            samples = tf.random.uniform(\n",
        "                shape=[batch_size, 2], maxval=self.vocab_size, dtype=tf.int32)\n",
        "            return tf.unstack(samples, axis=1)\n",
        "\n",
        "        def get_logits(x):\n",
        "            raw_logits = tf.matmul(self(x), tf.transpose(self.W))  # (B, V)\n",
        "            indices = tf.stack([tf.range(tf.shape(x)[0]), x], axis=1)\n",
        "            update = tf.zeros(tf.shape(x))\n",
        "            return tf.tensor_scatter_nd_update(raw_logits, indices, update)\n",
        "        x, y = pair\n",
        "        logits = get_logits(x) + get_logits(y)  # (B, V)\n",
        "        # Sample two samples by probability proportional to `exp(logits / self.T)`.\n",
        "        samples = tf.random.categorical((logits / self.T), 2, dtype=tf.int32)\n",
        "        return tf.unstack(samples, axis=1)\n",
        "\n",
        "    def loss(self, real_pair, fantasy_pair):\n",
        "        if TEST_WITHOUT_FANTASY_ENERGY:\n",
        "            return tf.reduce_mean(self.energy(real_pair))\n",
        "\n",
        "        return tf.reduce_mean(self.energy(real_pair) - self.energy(fantasy_pair))\n",
        "\n",
        "    def get_train_step(self, optimizer):\n",
        "        step = tf.Variable(0, dytpe=tf.int32)\n",
        "\n",
        "        @tf.function\n",
        "        def train_step(real_pair):\n",
        "            fantasy_pair = self.sample_fantasy(real_pair)\n",
        "\n",
        "            # Compute loss and its gradient, and optimize.\n",
        "            with tf.GradientTape() as gt:\n",
        "                loss_value = self.loss(real_pair, fantasy_pair)\n",
        "            grads = gt.gradient(loss_value, self.W)\n",
        "            # The gradient to the weights in embedding layer is treated as sparse,\n",
        "            # Convert sparse to dense for optimizer.\n",
        "            grads = tf.convert_to_tensor(grads)\n",
        "            optimizer.apply_gradients([(grads, self.W)])\n",
        "\n",
        "            step.assign_add(1)\n",
        "            return loss_value\n",
        "\n",
        "        return train_step, step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655",
      "metadata": {
        "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf",
      "metadata": {
        "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
        "optimizer = keras.optimizers.gradient_descent_v2.SGD()\n",
        "train_step, step = model.get_train_step(optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a7441b1c-eef8-484f-8533-f92b4bca2917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "a7441b1c-eef8-484f-8533-f92b4bca2917",
        "outputId": "e72ebfef-3c14-4a57-a83a-c8f984cc6930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227833/450835 [==============>...............] - ETA: 20:47 - loss: 0.0743"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-930e3e2b1782>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprocess_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProgbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreal_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     process_bar.update(current=tf.cast(step, tf.float32),\n\u001b[1;32m      5\u001b[0m                        values=[('loss', loss_value)])\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    130\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m   \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m   function = trace_function(\n\u001b[0m\u001b[1;32m    133\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    228\u001b[0m   )\n\u001b[1;32m    229\u001b[0m   lookup_func_type, lookup_func_context = (\n\u001b[0;32m--> 230\u001b[0;31m       function_type_utils.make_canonicalized_monomorphic_type(\n\u001b[0m\u001b[1;32m    231\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/function_type_utils.py\u001b[0m in \u001b[0;36mmake_canonicalized_monomorphic_type\u001b[0;34m(args, kwargs, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m   function_type, type_context = (\n\u001b[0;32m--> 375\u001b[0;31m       function_type_lib.canonicalize_to_monomorphic(\n\u001b[0m\u001b[1;32m    376\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolymorphic_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36mcanonicalize_to_monomorphic\u001b[0;34m(args, kwargs, default_values, capture_types, polymorphic_type)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m       parameters.append(\n\u001b[0;32m--> 583\u001b[0;31m           _make_validated_mono_param(name, arg, poly_parameter.kind,\n\u001b[0m\u001b[1;32m    584\u001b[0m                                      \u001b[0mtype_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                                      poly_parameter.type_constraint))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/polymorphism/function_type.py\u001b[0m in \u001b[0;36m_make_validated_mono_param\u001b[0;34m(name, value, kind, type_context, poly_type)\u001b[0m\n\u001b[1;32m    520\u001b[0m ) -> Parameter:\n\u001b[1;32m    521\u001b[0m   \u001b[0;34m\"\"\"Generates and validates a parameter for Monomorphic FunctionType.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m   \u001b[0mmono_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mpoly_type\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmono_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_subtype_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoly_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    168\u001b[0m           named_tuple_type, tuple(from_value(c, context) for c in value))\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    168\u001b[0m           named_tuple_type, tuple(from_value(c, context) for c in value))\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsTracingProtocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mgenerated_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tracing_type__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__tf_tracing_type__\u001b[0;34m(self, signature_context)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m     \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, dtype, name)\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mnot\u001b[0m \u001b[0mconvertible\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     \"\"\"\n\u001b[0;32m--> 866\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_shape_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShapeProto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munknown_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "process_bar = keras.utils.generic_utils.Progbar(len(ds.batch(BATCH_SIZE)))\n",
        "for real_pair in ds.shuffle(10000).batch(BATCH_SIZE):\n",
        "    loss_value = train_step(real_pair)\n",
        "    process_bar.update(current=tf.cast(step, tf.float32),\n",
        "                       values=[('loss', loss_value)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c66368ad-3d34-4f61-b4fb-8f30b764bd0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c66368ad-3d34-4f61-b4fb-8f30b764bd0b",
        "outputId": "0ee63363-249a-4793-837c-b03489215521"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'Variable:0' shape=(16384, 300) dtype=float32, numpy=\n",
              "array([[-2.0712823e-02,  1.6736245e-02, -7.9232091e-03, ...,\n",
              "         7.1018850e-03, -6.9449097e-03,  6.0067158e-03],\n",
              "       [-9.5178811e-03,  1.0509442e-02,  6.5404577e-03, ...,\n",
              "         6.4035687e-03, -2.5513403e-03,  1.4697091e-03],\n",
              "       [-1.8149236e-02, -1.3979237e-02,  9.5753688e-03, ...,\n",
              "        -3.1550520e-05,  1.0365978e-02,  1.0052994e-02],\n",
              "       ...,\n",
              "       [-2.2874910e-03,  3.4105316e-02, -3.4170862e-02, ...,\n",
              "        -7.7174106e-03, -7.6332660e-03, -1.6525479e-02],\n",
              "       [ 2.8279068e-02,  2.6577277e-02,  4.5417197e-02, ...,\n",
              "         2.1525720e-02, -4.1246541e-02, -2.3625584e-02],\n",
              "       [-2.4076367e-02, -6.1714659e-03,  2.4229636e-02, ...,\n",
              "         1.0513452e-02, -1.4421379e-02, -7.0991623e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.W"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc",
      "metadata": {
        "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fe99625-f249-4653-9650-ed946d7af46a",
      "metadata": {
        "id": "5fe99625-f249-4653-9650-ed946d7af46a"
      },
      "source": [
        "To evalute the model, we consider the $k$ words that are closest to a given word. The relation between these words shall be meaningful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8",
      "metadata": {
        "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8"
      },
      "outputs": [],
      "source": [
        "def get_closest_k(model, word, k):\n",
        "    vector = model([vocab[word]])  # (1, D)\n",
        "    distances = tf.matmul(vector, tf.transpose(model.W))  # (1, V)\n",
        "    _, top_ids = tf.math.top_k(distances, k=k)\n",
        "    return top_ids.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
        "outputId": "653ab659-2903-4b16-dd28-442ca2b1dd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "world: world, war, the, city, where\n",
            "\n",
            "boy: boy, builders, classically, breton, providence\n",
            "\n",
            "zero: zero, two, one, five, six\n",
            "\n",
            "sun: sun, acknowledged, decades, elevated, polished\n",
            "\n",
            "music: music, collegiate, secret, get, defining\n",
            "\n",
            "the: the, of, in, to, is\n",
            "\n",
            "to: to, the, be, this, have\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for word in ('world', 'boy', 'zero', 'sun', 'music', 'the', 'to'):\n",
        "    closest_indices = get_closest_k(model, word, 5)\n",
        "    print(f'{word}: {\", \".join([id_to_word[idx] for idx in closest_indices[0,:]])}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e82c7ea-a7a8-4e32-b281-1a28b5764271",
      "metadata": {
        "id": "9e82c7ea-a7a8-4e32-b281-1a28b5764271"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f",
      "metadata": {
        "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f"
      },
      "source": [
        "From this simple evaluation, it has been found that the word2vec re-implemented from the aspect of interaction reveals some deeper relations of words.\n",
        "\n",
        "If we drop the contribution in the loss from the fantasy data, the training fails in such a way that only the most frequent words (like \"the\", \"of\", and \"in\") appear as the closest for any word.\n",
        "\n",
        "If we sample fantasy data by uniform sampler, the training also fails in the previous way."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}