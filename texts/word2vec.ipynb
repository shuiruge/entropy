{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63cb55c7-31cd-48c2-a5e5-5faa728e7dbe",
   "metadata": {},
   "source": [
    "# Word2Vec as a Model of Interactions between Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24105460-c51c-42e1-a128-f3380d969b3a",
   "metadata": {},
   "source": [
    "## Word2Vec Introduction\n",
    "\n",
    "The [word2vec](https://en.wikipedia.org/wiki/Word2vec) model was proposed to vectorize words. A word is a string. It cannot be \"computed\" on a computer. We have to encode words to vectors. One direct way of encoding is called one-hot. That is, given a vocabulary which is a list of words, the i-th word has vector $x$ with $x^i$ = 1 and all other components vanish, like $(0, \\cdots, 0, 1, 0, \\cdots, 0)$. One-hot encoding is not very efficient since its dimension equals to the vocabulary size. But the vocabulary may be quite large. This motives the idea of word2vec, that is, encoding words to dense vectors with a small dimension.\n",
    "\n",
    "The basic idea behind word2vec is modeling the probability of appearance of two given words $w_1$ and $w_2$ as a neighbour in a corpus. Given two words $w_1$ and $w_2$ with vectors $x_1$ and $x_2$ respectively, the probability of being neighbour is assumed to be\n",
    "\n",
    "$$ p_{\\text{neighbour}} (w_1, w_2) \\propto \\exp(x_1 \\cdot x_2). $$\n",
    "\n",
    "So, for word2vec model, the learning task find a vector for each word so that the $p_{\\text{neighbour}}$ fits the real data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd748953-3543-48d4-9e54-060f949facc9",
   "metadata": {},
   "source": [
    "## Interactions between Words\n",
    "\n",
    "If words have been one-hot encoded, then for each one-hot encoded word $w_i$, its vector is given by $x_i = W \\cdot w_i$. The matrix $W$ has dimension $(E, V)$ where $E$ represents the word-vector dimension and $V$ the vocabulary size. Then, it can be derived directly that\n",
    "\n",
    "$$ x_1 \\cdot x_2 = u^t \\cdot A \\cdot u, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ u := w_1 + w_2 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ A := W^t \\cdot W - \\textrm{diag} (W^t \\cdot W). $$\n",
    "\n",
    "The matrix $A$ has dimension $(V, V)$. It is symmetric, with vanished diagonal elements. It is recognized as a Boltzmann machine with the energy given by $E(u; A) := -(1/2) u^t \\cdot A \\cdot u$ and unit temperature. Fitting a Boltzmann machine is minimizing the loss\n",
    "\n",
    "$$ L(W) = E(w_1, w_2; W) - E(\\tilde{w}_1, \\tilde{w}_2; W), $$\n",
    "\n",
    "for any two neighboured words (one-hot encoded) $(w_1, w_2)$ and two \"fantacy\" words $(\\tilde{w}_1, \\tilde{w}_2)$. The key point is ensuring that $E(w_1, w_2; W) > E(\\tilde{w}_1, \\tilde{w}_2; W)$ is more probable than the inverse. In this way, the $W$ is adjusted so that the $(w_1, w_2)$ is going to be a local minimum of the energy.\n",
    "\n",
    "Boltzmann machine ensures this by sampling $u' \\sim \\text{Bernoulli}(\\sigma(A \\cdot u))$, where $\\sigma$ is the sigmoid function. Generally, the sampled is not a two-hot vector, but multi-hot. The energy of $u'$ is the total energy of all the different pairs between the word-indices. The pairs may become too many to compute. We shall select only two word-indices from it. Notice that the greater $(A \\cdot u)_{\\alpha}$ is, the more probable we select word-index $\\alpha$. This hints for the categorical distribution\n",
    "\n",
    "$$ p_{\\alpha} = \\frac{ \\exp(\\sum_{\\beta} A_{\\alpha \\beta} u^{\\beta} / T) }{ \\sum_{\\alpha'} \\exp(\\sum_{\\beta'} A_{\\alpha' \\beta'} u^{\\beta'} / T) }, $$\n",
    "\n",
    "where $T$ is a positive number that characterizes the randomness. It is a categorical distribution with alphabet size $V$. It is indicated that we shall sample two word-indices based on this distribution as the fantasy data. It the fantasy energy does be no less than the real energy, the loss will be non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632b21f-312d-45f4-b1d5-58c84143313c",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94ac26b-3e24-4e29-b902-40b9b2cf0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9368e22f-2d85-42d9-a00f-50d3e0ce7481",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 2 ** 12  # vocabulary size.\n",
    "BATCH_SIZE = 128  # batch size of training.\n",
    "VECTOR_DIM = 150  # dimension of word-vector\n",
    "NEIGHBOURS = 2  # window size used for generating dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0afdc79-6b67-43c3-a7e5-9141930aa061",
   "metadata": {},
   "source": [
    "## Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b071814-9e64-403e-92c9-4c4a8afb2fa1",
   "metadata": {},
   "source": [
    "The original word2vec model is trained on the \"text8\" dataset. It is preprocessed and can be found on [internet](https://mattmahoney.net/dc/text8.zip). It is a zip file. By unzipping, we get a text file named `text8`. This is the preprocessed data that we can use directly, except for excluding words with single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7470af1c-07c8-4b33-a922-14fa6bd967b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/text8', 'r') as f:\n",
    "    text8 = [word for word in list(f)[0].split(' ') if len(word) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0419f6b8-683e-4a8d-bdc7-0bc96f6fbd53",
   "metadata": {},
   "source": [
    "There are lots of different words in the \"text8\" text. We shall limit the vocabulary used for building our model. For this purpose, we employ the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e8923-7f5e-409f-9799-bea21b472763",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counter = Counter(text8)\n",
    "vocab = {}\n",
    "for i, (word, _) in enumerate(counter.most_common(VOCAB_SIZE)):\n",
    "    vocab[word] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812e0c2-096e-483f-91bb-612ebf53c9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {i: w for w, i in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe13df0-dbe8-4e73-a445-de92a5c39b7f",
   "metadata": {},
   "source": [
    "Now, we construct the collection of pairs of center word (called \"target\" in the original paper of word2vec) and its neighbour (called \"context\") in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000eafbf-7070-4069-8a56-489981609308",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "targets = []\n",
    "contexts = []\n",
    "for (i, target) in enumerate(text8[NEIGHBOURS:-NEIGHBOURS]):\n",
    "    for j in range(i-NEIGHBOURS, i+NEIGHBOURS+1):\n",
    "        if j == i: continue\n",
    "        context = text8[j]\n",
    "        targets.append(target)\n",
    "        contexts.append(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d3ab3-de45-4358-83bd-1bd327064634",
   "metadata": {},
   "source": [
    "While converting from word to its index in the vocabulary, we have to drop the pairs in which there is at least one word that is absent in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7047d214-c345-460a-9785-1b6d9af51dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "target_ids = []\n",
    "context_ids = []\n",
    "for w1, w2 in zip(targets, contexts):\n",
    "    if w1 not in vocab or w2 not in vocab:\n",
    "        continue\n",
    "    target_ids.append(vocab[w1])\n",
    "    context_ids.append(vocab[w2])\n",
    "\n",
    "# List -> np.ndarray -> Dataset is much faster than List -> Dataset.\n",
    "target_ids = np.asarray(target_ids, dtype='int32')\n",
    "context_ids = np.asarray(context_ids, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9ede5-0d4d-4255-999e-cb45d5b731f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcdc584-5340-4ab9-82a3-f145b5eb7057",
   "metadata": {},
   "source": [
    "Now, convert the processed data to TensorFlow's dataset protocol for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa50fbc-f389-4ee5-9100-d89093030da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((target_ids, context_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b1562-7f18-40e4-9197-bd3fc66d46eb",
   "metadata": {},
   "source": [
    "Let see some instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640467e-80ac-432c-b6c1-de7c3f62f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in ds.batch(5).take(1):\n",
    "    tf.print('x: ', x)\n",
    "    tf.print('y: ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dccf72-b1f8-4b04-8eac-6086313d61a9",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482cf01-d6c7-4725-a6ea-f242c3ebd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    \"\"\"Word2Vec as a model of interactions between words.\n",
    "\n",
    "    Args:\n",
    "        vocab_size: Integer for the vocabulary size.\n",
    "        vector_dim: Integer for the word-vector dimension.\n",
    "        T: Positive float for the randomness in generating fantasy data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementation conventions:\n",
    "    # 1. The x and y employed throughout the implementation represent\n",
    "    #    word-indices. Thus they are tensors with shape [batch_size]\n",
    "    #    and dtype int32.\n",
    "    # 2. We use B for batch size, V for vocabulary size, and D for vector\n",
    "    #    dimension.\n",
    "\n",
    "    def __init__(self, vocab_size, vector_dim, T=1e-2):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vector_dim = vector_dim\n",
    "        self.T = T\n",
    "\n",
    "        self.W = tf.Variable(tf.random.uniform(\n",
    "            shape=[vocab_size, vector_dim],   # (V, D).\n",
    "            minval=-0.05, maxval=0.05,\n",
    "            dtype=tf.float32,\n",
    "        ))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.nn.embedding_lookup(self.W, x)\n",
    "\n",
    "    def energy(self, x, y):\n",
    "        return -tf.reduce_sum(self(x) * self(y), axis=1)\n",
    "\n",
    "    def sample_fantasy(self, x, y):\n",
    "        # Compute logits (the A \\cdot u, where u := x + y)\n",
    "        z = self(x) + self(y)  # (B, D)\n",
    "        raw_logits = tf.matmul(z, tf.transpose(self.W))  # (B, V)\n",
    "        indices = tf.stack([tf.range(tf.shape(x)[0]), x], axis=1)\n",
    "        update = tf.zeros(tf.shape(x))\n",
    "        logits = tf.tensor_scatter_nd_update(raw_logits, indices, update)\n",
    "\n",
    "        # Sample two samples by probability proportional to `exp(logits / self.T)`.\n",
    "        samples = tf.random.categorical((1/self.T) * logits, 2, dtype=tf.int32)\n",
    "        return tf.unstack(samples, axis=1)\n",
    "\n",
    "    def loss(self, real, fantasy):\n",
    "        return tf.reduce_mean(\n",
    "            self.energy(real[0], real[1]) -\n",
    "            self.energy(fantasy[0], fantasy[1])\n",
    "        )\n",
    "\n",
    "    def get_train_step(self, optimizer):\n",
    "        step = tf.Variable(0, dytpe=tf.int32)\n",
    "\n",
    "        @tf.function\n",
    "        def train_step(x, y):\n",
    "            \"\"\"The x and y are word-indices, tensors with shape [batch_size]\n",
    "            and dtype int32.\n",
    "            \"\"\"\n",
    "            real = (x, y)\n",
    "            fantasy = self.sample_fantasy(x, y)\n",
    "\n",
    "            # Compute loss and its gradient, and optimize.\n",
    "            # The gradient to the weights in embedding layer is treated as sparse,\n",
    "            # Convert sparse to dense for optimizer.\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss_value = self.loss(real, fantasy)\n",
    "            grads = tf.convert_to_tensor(tape.gradient(loss_value, self.W))\n",
    "            optimizer.apply_gradients([(grads, self.W)])\n",
    "\n",
    "            step.assign_add(1)\n",
    "            return loss_value\n",
    "\n",
    "        return train_step, step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0cbfb3-fe3e-466d-bb4a-d24105aaa655",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819361b-fc63-41f5-90a6-bd95d4ea4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(VOCAB_SIZE, VECTOR_DIM)\n",
    "# optimizer = keras.optimizers.gradient_descent_v2.SGD()\n",
    "optimizer = keras.optimizers.adam_v2.Adam()\n",
    "train_step, step = model.get_train_step(optimizer)\n",
    "# tf.profiler.experimental.start('logdir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7441b1c-eef8-484f-8533-f92b4bca2917",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_bar = keras.utils.generic_utils.Progbar(len(ds.batch(BATCH_SIZE)))\n",
    "for x, y in ds.shuffle(10000).batch(BATCH_SIZE):\n",
    "    loss_value = train_step(x, y)\n",
    "    process_bar.update(current=tf.cast(step, tf.float32),\n",
    "                       values=[('loss', loss_value)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66368ad-3d34-4f61-b4fb-8f30b764bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046f09ac-62ac-4e0b-b1eb-aab5d4b779cc",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe99625-f249-4653-9650-ed946d7af46a",
   "metadata": {},
   "source": [
    "Since the word-vectors are all normalized, it is natural to consider angular distance as a measurement of the relation between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117ac0d-4508-4c49-958d-bc18274c9dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_k(model, vector, k):\n",
    "    z = tf.convert_to_tensor([vector])  # (1, D)\n",
    "    distances = tf.math.acos(tf.matmul(z, tf.transpose(model.W)))  # (1, V)\n",
    "    _, top_ids = tf.math.top_k(-distances, k=k)\n",
    "    return top_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61245729-ccb5-4138-ae75-a2cfda616ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in ('world', 'boy', 'happy', 'zero', 'sun', 'football'):\n",
    "    closest_indices = get_closest_k(model, model(vocab[word]), 5)\n",
    "    print(f'{word}: {\", \".join([id_to_word[idx] for idx in closest_indices[0,:]])}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10581f-7ad9-4254-a9d3-1ae14de56f6f",
   "metadata": {},
   "source": [
    "From this simple evaluation, it has been found that the word2vec re-implemented from the aspect of interaction reveals some deeper relations of words.\n",
    "\n",
    "If we drop the contribution in the loss from the fantasy data, the training fails in such a way that only the most frequent words (like \"the\", \"of\", and \"in\") appear as the closest for any word.\n",
    "\n",
    "If we sample fantasy data by uniform sampler, the training also fails in the previous way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e3e11-9f51-4ec7-a8b0-f9eca47f1597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
