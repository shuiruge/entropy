{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07274ccd-76f1-46a0-b64d-29dd3d7b8019",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "In this notebook, we exame the idea declared in section 1.5.5 on Fashion MNIST data, a classification task.\n",
    "\n",
    "The model follows: https://www.kaggle.com/code/m0hand/fashion-mnist-cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8ab63f7-841b-4fe7-ae73-b0f8a207430e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 11:06:48.242273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.losses import MSE, CategoricalCrossentropy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import get_gradient_loss_fn\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8239d4-600f-4b3d-b665-ba64d9bac711",
   "metadata": {},
   "source": [
    "## The MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7ff4901-8ca8-4413-80ed-9184138a1ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([60000, 28, 28]), TensorShape([60000, 10]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = tf.convert_to_tensor(x_train / 255.0, 'float32')\n",
    "x_test = tf.convert_to_tensor(x_test / 255.0, 'float32')\n",
    "y_train = tf.one_hot(y_train, 10, dtype='float32')\n",
    "y_test = tf.one_hot(y_test, 10, dtype='float32')\n",
    "\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d08b9-61df-44d1-a10f-9b2297783c3b",
   "metadata": {},
   "source": [
    "## Train a Model with Gradient Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65e2d654-323d-43d2-805e-3a10fa1c28d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([ \n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, 'relu'),\n",
    "    Dense(10, 'softmax')\n",
    "])\n",
    "\n",
    "gradient_loss_fn = get_gradient_loss_fn(\n",
    "    lambda inputs: MSE(inputs[1], model(inputs[0]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0e9b1c-d29b-4ef7-8ede-d29b98e069d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = gradient_loss_fn((x, y))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd90d270-d554-4474-b95b-33c6826fee95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "    return accuracy_score(\n",
    "        tf.argmax(y_test, axis=1),\n",
    "        tf.argmax(model(x_test), axis=1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1992d379-bf40-4022-bcd6-6d26c2b757f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "ds = ds.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f8317b9-e6c9-4c30-b3c6-7036e6785d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:57<00:00, 10.40it/s]\n",
      "2024-03-20 11:07:48.193226: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1730560000 exceeds 10% of free system memory.\n",
      "2024-03-20 11:07:48.648614: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1730560000 exceeds 10% of free system memory.\n",
      "2024-03-20 11:07:49.077387: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1730560000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00072131696 0.8606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.68it/s]\n",
      "2024-03-20 11:08:46.445842: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1730560000 exceeds 10% of free system memory.\n",
      "2024-03-20 11:08:46.893912: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1730560000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.0005959251 0.8781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:55<00:00, 10.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.0005444534 0.8884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:55<00:00, 10.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.00049051974 0.8939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.00044895208 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:57<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.00045449394 0.8974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:58<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.0004537671 0.8982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:57<00:00, 10.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 0.00041171588 0.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:57<00:00, 10.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 0.0003834727 0.8996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:57<00:00, 10.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 0.00039342765 0.8937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.0003597469 0.8958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0.00035220583 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.00035183047 0.8967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 0.00024195388 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:55<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 0.00034668285 0.8917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.0002702052 0.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 0.0001871549 0.8984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 0.0002421062 0.8986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 0.00020391753 0.8991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 600/600 [00:56<00:00, 10.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 0.000275143 0.895\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    for x, y in tqdm(ds):\n",
    "        loss = train_step(x, y)\n",
    "    print(epoch, loss.numpy(), evaluate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dde9a7-4018-4e64-b909-e069729f096c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.895"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7218605-525e-445c-aa7a-0e44d3d39dc1",
   "metadata": {},
   "source": [
    "## Baseline Model with Usual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9993ada-54dd-4b1f-ab9e-3e3301695c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Sequential([ \n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=(28,28,1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, 'relu'),\n",
    "    Dense(10, 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dc98178-5940-4567-be22-6735de610d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=CategoricalCrossentropy(),\n",
    "    # loss=MSE,\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fe0e479-a012-4538-a6ca-132352299fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.4555 - accuracy: 0.8362 - val_loss: 0.3545 - val_accuracy: 0.8707\n",
      "Epoch 2/20\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.3054 - accuracy: 0.8885 - val_loss: 0.3500 - val_accuracy: 0.8710\n",
      "Epoch 3/20\n",
      "1875/1875 [==============================] - 26s 14ms/step - loss: 0.2625 - accuracy: 0.9028 - val_loss: 0.2981 - val_accuracy: 0.8909\n",
      "Epoch 4/20\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.2312 - accuracy: 0.9143 - val_loss: 0.2734 - val_accuracy: 0.9016\n",
      "Epoch 5/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.2089 - accuracy: 0.9223 - val_loss: 0.2677 - val_accuracy: 0.9044\n",
      "Epoch 6/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1868 - accuracy: 0.9301 - val_loss: 0.3002 - val_accuracy: 0.8955\n",
      "Epoch 7/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1705 - accuracy: 0.9358 - val_loss: 0.2748 - val_accuracy: 0.9067\n",
      "Epoch 8/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1537 - accuracy: 0.9426 - val_loss: 0.2796 - val_accuracy: 0.9052\n",
      "Epoch 9/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1413 - accuracy: 0.9474 - val_loss: 0.2731 - val_accuracy: 0.9095\n",
      "Epoch 10/20\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1306 - accuracy: 0.9511 - val_loss: 0.2906 - val_accuracy: 0.9052\n",
      "Epoch 11/20\n",
      "1875/1875 [==============================] - 27s 14ms/step - loss: 0.1169 - accuracy: 0.9562 - val_loss: 0.2825 - val_accuracy: 0.9120\n",
      "Epoch 12/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.1068 - accuracy: 0.9589 - val_loss: 0.3460 - val_accuracy: 0.9061\n",
      "Epoch 13/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0980 - accuracy: 0.9634 - val_loss: 0.3374 - val_accuracy: 0.9097\n",
      "Epoch 14/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0887 - accuracy: 0.9657 - val_loss: 0.3398 - val_accuracy: 0.9091\n",
      "Epoch 15/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0814 - accuracy: 0.9692 - val_loss: 0.3646 - val_accuracy: 0.9081\n",
      "Epoch 16/20\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0762 - accuracy: 0.9706 - val_loss: 0.3814 - val_accuracy: 0.9100\n",
      "Epoch 17/20\n",
      "1875/1875 [==============================] - 28s 15ms/step - loss: 0.0684 - accuracy: 0.9735 - val_loss: 0.3933 - val_accuracy: 0.9065\n",
      "Epoch 18/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0651 - accuracy: 0.9751 - val_loss: 0.4141 - val_accuracy: 0.9067\n",
      "Epoch 19/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0588 - accuracy: 0.9769 - val_loss: 0.4503 - val_accuracy: 0.9113\n",
      "Epoch 20/20\n",
      "1875/1875 [==============================] - 27s 15ms/step - loss: 0.0560 - accuracy: 0.9786 - val_loss: 0.4568 - val_accuracy: 0.9079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7faef1935350>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=20,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "637a961b-4f4b-4c9a-a5d3-b944c1ea01a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff6385a-4ca4-45fb-bf99-9953641fec71",
   "metadata": {},
   "source": [
    "## Model Robustness\n",
    "\n",
    "Now, we compare the robustness of the model and the baseline. To do so, we add Gaussian noise to the test data and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4719fdd-c6e1-4206-a2dd-cad3acae2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "stddev = 1e-1\n",
    "noise = tf.random.normal(tf.shape(x_test)) * stddev\n",
    "x_test_noised = x_test + noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8736769-691c-4d38-8d3b-a127b31664b7",
   "metadata": {},
   "source": [
    "Or, non-Gaussian \"pixal-flipping\" noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fbd2684-e1d2-4c72-bef5-b436ebc2b35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flip_ratio = 0.01\n",
    "# x_test_noised = tf.where(tf.random.uniform(tf.shape(x_test)) < flip_ratio, 1 - x_test, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41d56ef7-827f-433a-aa72-c939d5beffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_robustness(model):\n",
    "    accuracy = accuracy_score(\n",
    "        tf.argmax(y_test, axis=1), tf.argmax(model(x_test), axis=1))\n",
    "    accuracy_noised = accuracy_score(\n",
    "        tf.argmax(y_test, axis=1), tf.argmax(model(x_test_noised), axis=1))\n",
    "    print(f'Accuracy: {accuracy} -> {accuracy_noised}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20a92992-0d08-4255-9098-9f091690e48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.895 -> 0.8612\n"
     ]
    }
   ],
   "source": [
    "evaluate_robustness(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8d06e85-1c3a-4641-a73f-bf8a5c7383de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9079 -> 0.8206\n"
     ]
    }
   ],
   "source": [
    "evaluate_robustness(baseline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce4da5-feaf-42ea-a25a-cbf25b5fd89d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "- By simply using the \"gradient loss\", we obtained a result that approaches the baseline. But the robustness is greatly out-performs the baseline.\n",
    "- Because the \"gradient loss\" computes gradients twice (once by x and y, and once by model variables), the training duration is doubled.\n",
    "- We also tested non-Gaussian noise, such as flipping the given ratio of pixals. The result is that the baseline model turns to be more robust than the model trained by the \"gradient loss\". The difference is that the pixal-flipping noise is not continuous. The noised images jump to other local minima of the loss function.\n",
    "- We also use MSE as loss to train the baseline model. The final performance is a little worse than the categorical cross-entropy, as expected. The above conclusion is invariant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
